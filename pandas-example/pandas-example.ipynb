{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Pandas example\n",
    "\n",
    "> This guide assumes that you have set up TimescaleDB with TLSS to export Home Assistant data. For more info check [here](https://nghome.dev/docs/implementation/data_collection/timescale)\n",
    "\n",
    "This notebook shows how you can work with Pandas to manipulate and process data coming from Home Assistant.\n",
    "\n",
    "When you're going to use Pandas in your Home Automation setup you'll retrieve the needed data from your TimescaleDB Database. Seeing that this is not an option when working in a Google Colab notebook, I extracted some data from my database and made it available as a CSV that we can load in.\n",
    "If you are interested in how to retrieve the data from the database, check the full example at the bottom of this workbook.\n",
    "\n",
    "Good to note is that this notebook is meant as more of a hands-on showcase. I will use a lot of pandas functions that can be a bit confusing, please refer to the official [pandas docs](https://pandas.pydata.org/docs/) for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use case\n",
    "\n",
    "The use case of this notebook will be retrieving data from various sensors that measure metrics that have an impact on the temperature outdoor. Like sun azimuth and the amount of light coming in.\n",
    "\n",
    "I want to filter this data and make it useable for training a Machine Learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before we can start using Pandas we need to install the package first. Run the code block below to get pandas install through pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that we import the `pandas` package by running the code block below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving data\n",
    "\n",
    "Now that we have all prerequisites set up we can retrieve the prepared data from the CSV file.\n",
    "\n",
    "The data in this file has 3 columns:\n",
    "\n",
    "* `time` - The time at which the state change was recorded.\n",
    "* `entity_id` - The entity id of the specific sensor.\n",
    "* `state` - The recorded state of the sensor.\n",
    "\n",
    "We retrieve this CSV file by using the `read_csv()` file provided by pandas. This function reads the file and parses it into a `DataFrame` object.\n",
    "\n",
    "We then call the `head()` function on this DataFrame to show its first 5 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the data from CSV\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/moonen-home-automation/colab-notebooks/main/pandas-example/data/sensor_values.csv')\n",
    "# Show the first 5 items\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now see that the data has been retrieved successfully and the first 5 entries are presented to you.\n",
    "\n",
    "We can also retrieve a specific column of the DataFrame as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only retrieve the `state` column\n",
    "df[\"state\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the `time` column\n",
    "\n",
    "It may not be noticeable at first glance but the values in the `time` column are of type `str`, and not of type `datetime`. We need to convert these types for us to be able to effectively use the `time` data later on.\n",
    "\n",
    "Luckily pandas has a function called `to_datetime()` that takes in any time formatted string and tries to convert it to a `datetime` object.\n",
    "\n",
    "When this is done we can use the `dt.round()` function to round up the time values to the nearest minute. This makes the data easier to handle further on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the values of the time column and parse it into a `datetime` object\n",
    "df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "df[\"time\"] = df[\"time\"].dt.round(\"min\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the output the formatting of the time data has changed and they are rounded to the minute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating by time\n",
    "\n",
    "Now that we have the `time` column sorted and the values rounded. We can see that there is some overlap between entries. What I mean by this is that there are multiple entries that represent the same entity and the same timestamp.\n",
    "\n",
    "We can aggregate these rows together using pandas's `groupby()` and `agg()` functions.\n",
    "\n",
    "Take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the current amount of entries\n",
    "print(f\"current amount of entries: {df.shape[0]}\")\n",
    "\n",
    "# Group the entries together that have the same `time` adn `entity_id` values\n",
    "df_grouped = df.groupby([\"time\", \"entity_id\"])\n",
    "\n",
    "# Aggregate the values of these grouped entries\n",
    "# by calculating their mean (average) value\n",
    "# Also use `reset_index()` to reset the tables index so that entries are no longer grouped\n",
    "df_agg = df_grouped.agg({\"state\": \"mean\"}).reset_index()\n",
    "\n",
    "# Print the amount of entries after aggregation\n",
    "print(f\"amount after aggregation: {df_agg.shape[0]}\")\n",
    "df_agg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see that the amount of entries drastically decreased and that we now have a way cleaner dataset to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivoting the DataFrame\n",
    "\n",
    "Now in order to group readings made at the same time together we can pivot this DataFrame.\n",
    "\n",
    "Where before we had a the columns `time`, `entity_id`, and `state.\n",
    "\n",
    "I now want to give each occurrence of `entity_id` its own column, with the state of that entity as its value. We keep the `time` column in place.\n",
    "\n",
    "This may sound a bit confusing, but run the code block below and compare the table with the ones above. It will become more clear then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the table using the `time` value as the index,\n",
    "# separate out the `entity_id` values into columns,\n",
    "# and use the `state` values\n",
    "df_pivot = df_agg.pivot(index=\"time\", columns=\"entity_id\", values=\"state\")\n",
    "\n",
    "df_pivot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we now have rows representing all `time` entries, and the state of the sensors at that specific time.\n",
    "\n",
    "You'll maybe notice that there's still something wrong with this table. The naming of the columns is a bit messed up. And the table has lost its numeric index. Let fix that with the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index of the table to a numeric index\n",
    "df_pivot.reset_index(inplace=True)\n",
    "# Remote the unnecessary `entity_id` columns name\n",
    "df_pivot.columns.name = None\n",
    "\n",
    "df_pivot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's looking way better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping empty values\n",
    "\n",
    "We are almost done playing with our data, but as you may have noticed there are some null values in the table. These are shown as `NaN`, the rows containing this values are of no use to us and therefor need to be dropped. Luckily pandas has a easy function for that: `dropna()`. This will drop any rows containing null values from the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where any value is NaN\n",
    "df_filtered = df_pivot.dropna(how=\"any\")\n",
    "\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the `NaN` values are no longer present in the table. Those rows have been removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final data\n",
    "\n",
    "And thats it...\n",
    "\n",
    "Our data is now ready for training a ML model.\n",
    "\n",
    "Now when you are going to use this data you should store it somewhere so you can use it for training later. Luckily pandas has some tools for storing you data in various places. Check the at the end of this notebook to see how.\n",
    "\n",
    "We can see how many of the original rows are left after we parsed, aggregated, pivoted, and filtered the data by running the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"original amount of entries: {df.shape[0]}\")\n",
    "\n",
    "print(f\"amount of entries left: {df_filtered.shape[0]}\")\n",
    "\n",
    "pct_left = (df_filtered.shape[0] / df.shape[0]) * 100\n",
    "print(f\"that's {int(pct_left)}% of the original amount\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-world example\n",
    "\n",
    "Now below you will find a real world example of how this will be used. Loading the data from the Timescale database, manipulating it, and then putting it back in its own table for later use.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Setup connection to TimescaleDB\n",
    "connection_string = \"postgresql://postgres:***@hassio.local:5432/homeassistant\"\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "df = pd.read_sql_query(\n",
    "    \"SELECT time, entity_id, state from ltss WHERE entity_id='sensor.indoor_lux' OR entity_id='sensor.outdoor_temp' OR entity_id='sensor.sun_azimuth'\",\n",
    "    engine,\n",
    ")\n",
    "\n",
    "df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "df[\"time\"] = df[\"time\"].dt.round(\"min\")\n",
    "df[\"state\"] = pd.to_numeric(df[\"state\"], errors=\"coerce\")\n",
    "\n",
    "df_agg = df.groupby([\"time\", \"entity_id\"]).agg({\"state\": \"mean\"}).reset_index()\n",
    "\n",
    "df_pivot = df_agg.pivot(index=\"time\", columns=\"entity_id\", values=\"state\")\n",
    "df_pivot.reset_index(inplace=True)\n",
    "df_pivot.columns.name = None\n",
    "\n",
    "df_filtered = df_pivot.dropna(how=\"any\")\n",
    "\n",
    "# Write back data to a table called `ml_data`\n",
    "df_final.to_sql(\"ml_data\", engine, if_exists=\"replace\", index=False)\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
